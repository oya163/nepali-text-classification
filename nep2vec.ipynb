{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reading XML and TXT corpus simultaneously"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "    Generates word2vec for Nepali corpus\n",
    "    \n",
    "    Author - Oyesh Mann Singh\n",
    "    Date - 02/26/2019\n",
    "    \n",
    "    For NNC corpus data:\n",
    "        https://www.sketchengine.eu/nepali-national-corpus/\n",
    "        or,\n",
    "        Contact - osingh1@umbc.edu\n",
    "'''\n",
    "\n",
    "import sys, os\n",
    "import unicodedata\n",
    "import nltk\n",
    "import nltk.corpus as nc\n",
    "import NNCCorpus as nnc\n",
    "import string\n",
    "import itertools\n",
    "import csv\n",
    "import numpy as np\n",
    "\n",
    "xmlDir = \"../data/nnc/\"\n",
    "xml = nnc.NNCCorpusReader(xmlDir, fileids=r'(?!\\.).*\\.xml')\n",
    "\n",
    "combinedDir = \"../data/ner/bal/text_tag_only/\"\n",
    "combineTxt = nc.IndianCorpusReader(combinedDir, fileids='text_only.txt')\n",
    "\n",
    "# newsDir = \"../data/clean_corpus/\"\n",
    "# news = nc.IndianCorpusReader(newsDir, fileids=r'(?!\\.).*\\.txt')\n",
    "\n",
    "# suryaDir = \"../data/ner/NER_surya_bam/raw_data/\"\n",
    "# surya = nc.IndianCorpusReader(suryaDir, fileids=r'(?!\\.).*\\.txt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prepare dictionary to remove unnecessary unicode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "    XML corpus clean up\n",
    "    \n",
    "    Need to look more closely on Nepali Preprocessing\n",
    "    for word2vec.\n",
    "    \n",
    "    Currently removing everything for simplicity except hyphen and period\n",
    "\n",
    "    Lu = Latin uppercase\n",
    "    Ll = Latin lowercase\n",
    "    P = Punctutation\n",
    "    N = Number\n",
    "    S = Symbol\n",
    "    Cf = Other, format\n",
    "    Cn = Other, not assigned\n",
    "    Cc = Other, control\n",
    "    Lo = Letter Other\n",
    "\n",
    "'''\n",
    "\n",
    "# Not removing HYPHEN and FULL STOP\n",
    "# For Danda i != 2404\n",
    "\n",
    "stem_file=open('../data/ner/stemming/new_postposition.txt', 'r', encoding='utf-8')\n",
    "stemmers=stem_file.readlines()[0].split()\n",
    "\n",
    "def stemmer(sentence):\n",
    "    lemma_tag='O'\n",
    "    not_to_be_lemmatized=['एमाले', 'अमेरिका', 'अधिकारी', 'शङ्का', 'मात्रिका']\n",
    "    stemmed_sentence=[]\n",
    "    for words in sentence:\n",
    "        saved_pp=''\n",
    "        lemmatize=False\n",
    "        for pp in stemmers:\n",
    "            if words == pp:\n",
    "                break            \n",
    "            elif words.endswith(pp):\n",
    "                words=words[:-len(pp)]\n",
    "                saved_pp = pp\n",
    "                lemmatize=True\n",
    "                break\n",
    "        if len(words) > 0:\n",
    "            stemmed_sentence.append(words)        \n",
    "            if lemmatize:\n",
    "                stemmed_sentence.append(saved_pp)\n",
    "                lemmatize=False\n",
    "    return stemmed_sentence\n",
    "\n",
    "\n",
    "table = dict.fromkeys(i for i in range(sys.maxunicode) \n",
    "                        if unicodedata.category(chr(i)).startswith(('Lu', 'Ll', 'Cf','Cn','Cc'))\n",
    "                        and i != 45 and i != 46 and i != 2404)\n",
    "\n",
    "final_sents = []\n",
    "\n",
    "# Convert to one-string format, remove punctuations, split to string word-wise\n",
    "def read_corpus(data, stem=False, process=False):\n",
    "    finals=[]\n",
    "    for sent in data.sents():\n",
    "        if process:\n",
    "            sent = ' '.join(sent).translate(table).split()\n",
    "        if stem:\n",
    "            sent=stemmer(sent)\n",
    "        finals.append(sent)\n",
    "    return finals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<timed exec>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-2-a505336259c8>\u001b[0m in \u001b[0;36mread_corpus\u001b[0;34m(data, stem, process)\u001b[0m\n\u001b[1;32m     57\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mread_corpus\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstem\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprocess\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     58\u001b[0m     \u001b[0mfinals\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 59\u001b[0;31m     \u001b[0;32mfor\u001b[0m \u001b[0msent\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msents\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     60\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mprocess\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     61\u001b[0m             \u001b[0msent\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m' '\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msent\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtranslate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtable\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/oya-nepali-nlp/classification/NNCCorpus.py\u001b[0m in \u001b[0;36msents\u001b[0;34m(self, fileids, strip_space, stem)\u001b[0m\n\u001b[1;32m     77\u001b[0m         \u001b[0;34m:\u001b[0m\u001b[0mparam\u001b[0m \u001b[0mstem\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mIf\u001b[0m \u001b[0mtrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mthen\u001b[0m \u001b[0muse\u001b[0m \u001b[0mword\u001b[0m \u001b[0mstems\u001b[0m \u001b[0minstead\u001b[0m \u001b[0mof\u001b[0m \u001b[0mword\u001b[0m \u001b[0mstrings\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     78\u001b[0m         \"\"\"\n\u001b[0;32m---> 79\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_views\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfileids\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstrip_space\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstem\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     80\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     81\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/oya-nepali-nlp/classification/NNCCorpus.py\u001b[0m in \u001b[0;36m_views\u001b[0;34m(self, fileids, sent, tag, strip_space, stem)\u001b[0m\n\u001b[1;32m    100\u001b[0m         \u001b[0;34m\"\"\"A helper function that instantiates NNCWordViews or the list of words/sentences.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    101\u001b[0m         \u001b[0mf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mNNCWordView\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_lazy\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_words\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 102\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mconcat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfileid\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msent\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtag\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstrip_space\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstem\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mfileid\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mabspaths\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfileids\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    103\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    104\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_words\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfileid\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbracket_sent\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtag\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstrip_space\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstem\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/oya-nepali-nlp/classification/NNCCorpus.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    100\u001b[0m         \u001b[0;34m\"\"\"A helper function that instantiates NNCWordViews or the list of words/sentences.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    101\u001b[0m         \u001b[0mf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mNNCWordView\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_lazy\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_words\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 102\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mconcat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfileid\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msent\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtag\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstrip_space\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstem\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mfileid\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mabspaths\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfileids\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    103\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    104\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_words\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfileid\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbracket_sent\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtag\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstrip_space\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstem\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/oya-nepali-nlp/classification/NNCCorpus.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, fileid, sent, tag, strip_space, stem)\u001b[0m\n\u001b[1;32m    207\u001b[0m         \u001b[0;31m# Read in a tasty header.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    208\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_open\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 209\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_block\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_stream\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'.*/teiHeader$'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhandle_header\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    210\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    211\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/lib/python3.6/site-packages/nltk/corpus/reader/xmldocs.py\u001b[0m in \u001b[0;36mread_block\u001b[0;34m(self, stream, tagspec, elt_handler)\u001b[0m\n\u001b[1;32m    333\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    334\u001b[0m                 \u001b[0;32melif\u001b[0m \u001b[0mpiece\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgroup\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'END_TAG'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 335\u001b[0;31m                     \u001b[0mname\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_XML_TAG_NAME\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmatch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpiece\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgroup\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgroup\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    336\u001b[0m                     \u001b[0;31m# sanity checks:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    337\u001b[0m                     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mcontext\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# Read NNC corpus only\n",
    "'''\n",
    "    STEM=True might not be necessary because NNC corpus is already lemmatized\n",
    "'''\n",
    "nnc_sents=read_corpus(xml, stem=False, process=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 56 ms, sys: 0 ns, total: 56 ms\n",
      "Wall time: 55.3 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "'''\n",
    "    STEM=True might not be necessary if the data source is after_stemming/text_tag_only/text_only.txt\n",
    "'''\n",
    "dataset_sents=read_corpus(combineTxt, stem=False, process=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'nnc_sents' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-6-5d97d5322dec>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mfinal_sents\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdataset_sents\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mnnc_sents\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'nnc_sents' is not defined"
     ]
    }
   ],
   "source": [
    "final_sents = dataset_sents + nnc_sents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For fasttext embedding\n",
    "import string\n",
    "with open('../data/fasttext/nep2ft-corpus-bal.txt', 'w', encoding='utf-8') as outfile:\n",
    "    for each in final_sents:\n",
    "        outfile.write(' '.join(each))\n",
    "        outfile.write('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of sentences 804002\n",
      "Total number of words 14592452\n"
     ]
    }
   ],
   "source": [
    "num_of_words = 0\n",
    "for each_sent in final_sents:\n",
    "    num_of_words += len(each_sent)\n",
    "    \n",
    "    \n",
    "print(\"Total number of sentences\", len(final_sents))\n",
    "print(\"Total number of words\", num_of_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%time\n",
    "# # For glove, we need to make plain txt file\n",
    "# final_sents = list(itertools.chain.from_iterable(final_sents))\n",
    "\n",
    "# with open('../data/glove/nepali_raw.txt', 'w', encoding='utf-8') as f:\n",
    "#     for item in final_sents:\n",
    "#         f.write(\"%s\\n\" % item)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create Word2Vec model and Load it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "from gensim.models import Word2Vec, KeyedVectors\n",
    "\n",
    "model = Word2Vec(final_sents, size=300, min_count=5, window=10, sg=0, workers=10)\n",
    "# model = Word2Vec.load('./large_nep2vec.model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Vocabulary dimension\", model.vector_size)\n",
    "print(\"Number of words in vocab\", len(model.wv.vocab))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save Word2Vec model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "### Save Word2Vec model\n",
    "\n",
    "from gensim.test.utils import get_tmpfile\n",
    "\n",
    "# model_path = '../data/ner/nep2vec.model'\n",
    "# Save new model\n",
    "model.wv.save_word2vec_format('../data/nep2vec/nep2vec_stem-cbow', binary=False)\n",
    "# model.save(model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from gensim.models import Word2Vec, KeyedVectors\n",
    "\n",
    "# model = KeyedVectors.load_word2vec_format('../data/nep2vec/nep2vec_text_stem', binary=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check for word similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('बोक्रा', 0.8281864523887634),\n",
       " ('पात', 0.827208399772644),\n",
       " ('झोल', 0.8253916501998901),\n",
       " ('गोबर', 0.8253674507141113),\n",
       " ('धागो', 0.819855809211731),\n",
       " ('हरियो', 0.8166791796684265),\n",
       " ('दाना', 0.816335916519165),\n",
       " ('बोटबिरुवा', 0.8141928911209106),\n",
       " ('प्लास्टिक', 0.8104241490364075),\n",
       " ('लसुन', 0.8090667128562927)]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.wv.most_similar('धूलो', topn=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
